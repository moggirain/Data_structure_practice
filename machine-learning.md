# Machine Learning

**What is regularization?** 

* Regularization control the values of the parameter through adding the penalty term to the objective function, in order to avoid constrain a model to make it simpler and reduce the risk of overfitting. 
* This is due to the bias variance trade-off, that is, the model is complex, the model fits the training model so well, so the bias is very and and the variance is very high, but it will have poor performance on testing data set, so the generalization of the model is poor. We need to use the regularization to decrease the variance with scarifying the bias. 
* Large reduction in the variance 
* L1 and L2 
* Non parametric: Early Stopping regularization methods 
* What is the effect of L2? 
* Compress the coefficient, and silmutaneously 
* Bayesian optimization and 
*  Why regularization ends up in smaller weights? 
*  What is a hyperparameter? Give an example.

          - hyper-parameter is the parameter that can not be trained in the learning process. 

* What is grid search? 
*  * What is the process of performing hyper parameter tuning in order to determine the optimal values for a given model? 
  * evaluate a model for each combination of **algorithm** parameters specified in a **grid**.
  * Bayesian optimization
* What other methods can you use to determine hyperparameter？ 
* * Then what is the advantage? 
* When we talk about gradient descent, what is it? 
* Under what circumstance can you find the global min?
*  How can we avoid overshooting? 
* What is random forest and the difference from a tree? 
* bagging: decrease variance not bias and increase the model performance 
* Add the randomness of the samples, tree and tree are  uncorrelated. But bootstrapping makes the tree interrelated. 
* Subset of variables and the bias might increase. 
* parallel computing 
* Bagging only works for tree model 
* bagging disadvantage: 
* What is boosting? 
* model the sampling distribution and not iid 
* boosting model performance is 
* not parallel computing and boosting is slow 
* What is KNN?
* lazy learner, 
* What is clustering? 
* The concepts of precision, recall, f1? 
* How to deal with imbalanced data?
* sampling 
* What can you do if you can get more data?
* Why can't a model work forever?
* 1） The underlying distribution of data changed.
* 2） Data collection and its samples representative. Solution: Monitor your model and automatically evaluate the metrics. 
* cosine similarity 

What methods are you using to over

-[https://towardsdatascience.com/all-about-missing-data-handling-b94b8b5d2184](https://towardsdatascience.com/all-about-missing-data-handling-b94b8b5d2184)

* normalize the data before 
* [https://machinelearningmastery.com/what-is-bayesian-optimization/](https://machinelearningmastery.com/what-is-bayesian-optimization/)

